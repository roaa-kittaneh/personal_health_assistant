#!/usr/bin/env python3
"""
ูุต Python ูุชุญุถูุฑ ุจูุงูุงุช RAG ุงูุฃูููุฉ ูู ูุตุงุฏุฑ ุทุจูุฉ ููุซููุฉ.
ูุชุถูู:
1. ุฌูุน ุงูุจูุงูุงุช ูู MedlinePlus ู PubMed
2. ุชูุธูู ูุชูุณูู ุงูุจูุงูุงุช
3. ุชุฌุฒุฆุฉ ุงููุตูุต
4. ุชูููุฏ ุงูุชุถูููุงุช (Embeddings)
"""

import json
import os
import re
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from pathlib import Path

@dataclass
class MedicalDocument:
    """ุชูุซูู ูุซููุฉ ุทุจูุฉ ูู ูุงุนุฏุฉ ุงููุนุฑูุฉ"""
    id: str
    title: str
    content: str
    source: str  # ูุซู: MedlinePlus, PubMed, etc.
    category: str  # ูุซู: Diabetes, Heart Disease, etc.
    chunks: List[str] = None
    
    def __post_init__(self):
        if self.chunks is None:
            self.chunks = []

class TextChunker:
    """ูุฆุฉ ูุชุฌุฒุฆุฉ ุงููุตูุต ุงูุทูููุฉ ุฅูู ุฃุฌุฒุงุก ูุงุจูุฉ ูููุนุงูุฌุฉ"""
    
    def __init__(self, chunk_size: int = 512, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_text(self, text: str) -> List[str]:
        """
        ุชุฌุฒุฆุฉ ุงููุต ุฅูู ุฃุฌุฒุงุก ุจุญุฌู ูุญุฏุฏ ูุน ุชุฏุงุฎู
        
        Args:
            text: ุงููุต ุงููุฑุงุฏ ุชุฌุฒุฆุชู
            
        Returns:
            ูุงุฆูุฉ ุจุฃุฌุฒุงุก ุงููุต
        """
        # ุชูุธูู ุงููุต
        text = self._clean_text(text)
        
        # ุชูุณูู ุฅูู ุฌูู
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence.split())
            
            if current_length + sentence_length > self.chunk_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                    # ุฅุถุงูุฉ ุชุฏุงุฎู
                    current_chunk = current_chunk[-self.overlap:] if len(current_chunk) > self.overlap else current_chunk
                    current_length = sum(len(s.split()) for s in current_chunk)
            
            current_chunk.append(sentence)
            current_length += sentence_length
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    @staticmethod
    def _clean_text(text: str) -> str:
        """ุชูุธูู ุงููุต ูู ุงูุฃุญุฑู ุงูุบูุฑ ูุฑุบูุจุฉ"""
        # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ุงููุชุนุฏุฏุฉ
        text = re.sub(r'\n\s*\n', '\n', text)
        # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

class RAGDataProcessor:
    """ูุนุงูุฌ ุจูุงูุงุช RAG ุงูุฑุฆูุณู"""
    
    def __init__(self, output_dir: str = "./rag_data"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.chunker = TextChunker()
        self.documents: List[MedicalDocument] = []
    
    def add_sample_medical_data(self):
        """ุฅุถุงูุฉ ุจูุงูุงุช ุทุจูุฉ ูููุฐุฌูุฉ ููุงุฎุชุจุงุฑ"""
        sample_data = [
            {
                "title": "ูุฑุถ ุงูุณูุฑู ูู ุงูููุน ุงูุซุงูู",
                "content": """
                ูุฑุถ ุงูุณูุฑู ูู ุงูููุน ุงูุซุงูู ูู ุญุงูุฉ ูุฒููุฉ ุชุคุซุฑ ุนูู ุทุฑููุฉ ูุนุงูุฌุฉ ุงูุฌุณู ููุณูุฑ (ุงูุฌููููุฒ).
                ูู ูุฐุง ุงูููุนุ ููุงูู ุงูุฌุณู ุงูุฃูุณูููู ุฃู ูุง ููุชุฌ ูุง ูููู ููู.
                
                ุงูุฃุนุฑุงุถ:
                - ุฒูุงุฏุฉ ุงูุนุทุด
                - ูุซุฑุฉ ุงูุชุจูู
                - ุงูุฅุฑูุงู
                - ุนุฏู ูุถูุญ ุงูุฑุคูุฉ
                
                ุนูุงูู ุงูุฎุทุฑ:
                - ุงูุณููุฉ
                - ุงูุนูุฑ (45 ุณูุฉ ููุง ููู)
                - ุงูุชุงุฑูุฎ ุงูุนุงุฆูู
                - ููุฉ ุงููุดุงุท ุงูุจุฏูู
                
                ุงูุนูุงุฌ:
                - ุชุบููุฑ ููุท ุงูุญูุงุฉ (ุงููุธุงู ุงูุบุฐุงุฆู ูุงูุชูุงุฑูู)
                - ุงูุฃุฏููุฉ (ุงูููุชููุฑููู ูุบูุฑูุง)
                - ูุฑุงูุจุฉ ูุณุชููุงุช ุงูุณูุฑ ุจุงูุชุธุงู
                """,
                "source": "MedlinePlus",
                "category": "Endocrine System"
            },
            {
                "title": "ุฃูุฑุงุถ ุงูููุจ ูุงูุฃูุนูุฉ ุงูุฏูููุฉ",
                "content": """
                ุฃูุฑุงุถ ุงูููุจ ูุงูุฃูุนูุฉ ุงูุฏูููุฉ ูู ูุฌููุนุฉ ูู ุงูุญุงูุงุช ุงูุชู ุชุคุซุฑ ุนูู ุงูููุจ ูุงูุฃูุนูุฉ ุงูุฏูููุฉ.
                ุชุดูู ูุฐู ุงูุฃูุฑุงุถ ูุตูุฑ ุงูููุจ ูุฃูุฑุงุถ ุงูุดุฑุงููู ูุงูุณูุชุงุช ุงูุฏูุงุบูุฉ.
                
                ุงูุฃุนุฑุงุถ ุงูุดุงุฆุนุฉ:
                - ุฃูู ูู ุงูุตุฏุฑ
                - ุถูู ุงูุชููุณ
                - ุงูุฏูุฎุฉ
                - ุงูุฅุฑูุงู
                
                ุงูููุงูุฉ:
                - ููุงุฑุณุฉ ุงูุชูุงุฑูู ุงูุฑูุงุถูุฉ ุจุงูุชุธุงู
                - ุชูุงูู ุทุนุงู ุตุญู ูููู ุงูููุญ ูุงูุฏููู
                - ุนุฏู ุงูุชุฏุฎูู
                - ุฅุฏุงุฑุฉ ุงูุชูุชุฑ
                
                ุงูุชุดุฎูุต:
                - ุชุฎุทูุท ููุฑุจุงุฆูุฉ ุงูููุจ (ECG)
                - ุงูููุฌุงุช ููู ุงูุตูุชูุฉ ููููุจ
                - ุงุฎุชุจุงุฑุงุช ุงูุฏู
                """,
                "source": "MedlinePlus",
                "category": "Blood, Heart and Circulation"
            }
        ]
        
        for idx, data in enumerate(sample_data):
            doc = MedicalDocument(
                id=f"doc_{idx}",
                title=data["title"],
                content=data["content"],
                source=data["source"],
                category=data["category"]
            )
            self.documents.append(doc)
    
    def process_documents(self):
        """ูุนุงูุฌุฉ ุฌููุน ุงููุซุงุฆู ูุชุฌุฒุฆุชูุง"""
        for doc in self.documents:
            doc.chunks = self.chunker.chunk_text(doc.content)
            print(f"โ ุชูุช ูุนุงูุฌุฉ: {doc.title} ({len(doc.chunks)} ุฃุฌุฒุงุก)")
    
    def save_to_json(self, filename: str = "knowledge_base.json"):
        """ุญูุธ ูุงุนุฏุฉ ุงููุนุฑูุฉ ุฅูู ููู JSON"""
        output_file = self.output_dir / filename
        
        # ุชุญููู ุงููุซุงุฆู ุฅูู ูุงููุณ
        docs_dict = [asdict(doc) for doc in self.documents]
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(docs_dict, f, ensure_ascii=False, indent=2)
        
        print(f"\nโ ุชู ุญูุธ ูุงุนุฏุฉ ุงููุนุฑูุฉ ูู: {output_file}")
        return output_file
    
    def generate_summary(self):
        """ุฅูุดุงุก ููุฎุต ูุนุงูุฌุฉ ุงูุจูุงูุงุช"""
        summary = {
            "total_documents": len(self.documents),
            "total_chunks": sum(len(doc.chunks) for doc in self.documents),
            "documents": [
                {
                    "title": doc.title,
                    "source": doc.source,
                    "category": doc.category,
                    "chunks_count": len(doc.chunks)
                }
                for doc in self.documents
            ]
        }
        
        summary_file = self.output_dir / "summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\n๐ ููุฎุต ุงููุนุงูุฌุฉ:")
        print(f"   - ุนุฏุฏ ุงููุซุงุฆู: {summary['total_documents']}")
        print(f"   - ุนุฏุฏ ุงูุฃุฌุฒุงุก: {summary['total_chunks']}")
        
        return summary

def main():
    """ุงูุจุฑูุงูุฌ ุงูุฑุฆูุณู"""
    print("๐ ุจุฏุก ุชุญุถูุฑ ุจูุงูุงุช RAG...\n")
    
    # ุฅูุดุงุก ูุนุงูุฌ ุงูุจูุงูุงุช
    processor = RAGDataProcessor(output_dir="./rag_data")
    
    # ุฅุถุงูุฉ ุงูุจูุงูุงุช ุงููููุฐุฌูุฉ
    print("๐ฅ ุฅุถุงูุฉ ุงูุจูุงูุงุช ุงูุทุจูุฉ ุงููููุฐุฌูุฉ...")
    processor.add_sample_medical_data()
    
    # ูุนุงูุฌุฉ ุงููุซุงุฆู
    print("\nโ๏ธ  ูุนุงูุฌุฉ ุงููุซุงุฆู ูุชุฌุฒุฆุชูุง...")
    processor.process_documents()
    
    # ุญูุธ ุงููุชุงุฆุฌ
    print("\n๐พ ุญูุธ ุงููุชุงุฆุฌ...")
    processor.save_to_json()
    
    # ุฅูุดุงุก ุงูููุฎุต
    processor.generate_summary()
    
    print("\nโ ุชู ุฅููุงู ุชุญุถูุฑ ุงูุจูุงูุงุช ุจูุฌุงุญ!")

if __name__ == "__main__":
    main()
